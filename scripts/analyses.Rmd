---
title: "analyses"
author: "Alejandra Garcia Isaza"
date: "4/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rio)
library(here)
library(tidyverse)
library(haven)
library(janitor)
library(knitr)
library(surveytoolbox)
library(sjPlot)
library(kableExtra)
library(psych)
library(sjmisc)
library(MVN)

theme_set(theme_minimal())
```

# Loading the dataset
```{r}
d <- read_sav(here("data", "parent_hv1_youth_w1.sav"))
```

# only analysis variables

## id variables
school_id, condition, family_id, participant_id

## scales variables
- Structure at home (8)
select(q23_p1:q30_p1)

- School-Based Involvement (10)
select(q31_p1:q40_p1)

- Parent Belongingness in School (7)
select(q47_p1, q48_p1, q51_p1, q52_p1, q53_p1, q59_p1, q60_p1)

- Parent Endorsement of School (4)
select(q54_p1:q57_p1)

- Parent-Teacher Relationship (4) 
select(q64_p1:q67_p1)

- Parent-Child Conversations About School (14)
select(q68_p1:q81_p1)

- Homework Involvement (17)
select(q82_p1:q98_p1)

- Parent’s Value and Support of Education (6)
select(q105_p1:q110_p1)

- Appropriate Discipline (7)
select(q128_p1:q131_p1, q133_p1, q134_p1, q136_p1)

- Monitoring (5)
select(q137_p1, q139_p1, q140_p1, q143_p1, q144_p1)

- Family-School Communication (6)
select(q18_p1, q19_p1, q41_p1, q43_p1, q50_p1, q62_p1)

- Problem Solving with Educators (4)
select(q44_p1, q58_p1, q63_p1, q126_p1)

## outcome variable
- Students’ School Engagement (9)
select(q83_y1:q91_y1)

## moderators
- Ed level (1 - 11)
q36_hv1

- ENG comfort in youth school (1 - 5, 77, 99)
q173_3_p1

```{r}
d1 <- d %>%
  select(school_id, condition, family_id, participant_id, q23_p1:q30_p1, q31_p1:q40_p1, q47_p1, q48_p1, q51_p1, q52_p1, q53_p1, q59_p1, q60_p1, q54_p1:q57_p1, q64_p1:q67_p1, q68_p1:q81_p1, q82_p1:q98_p1, q105_p1:q110_p1, q128_p1:q131_p1, q133_p1, q134_p1, q136_p1, q137_p1, q139_p1, q140_p1, q143_p1, q144_p1, q18_p1, q19_p1, q41_p1, q43_p1, q50_p1, q62_p1, q44_p1, q58_p1, q63_p1, q126_p1, q36_hv1, q173_3_p1, q83_y1:q91_y1) # double check this, evaluate missing data 
```

# descriptives check 
```{r}
describe(d1)# using describe function I identified that missing values in the dataset were -99, 99, 77
```


# recoding missing variables as N/A
```{r include=FALSE}
# recoding missing values as N/A with function

# vector with missing values in dataset
missing_vals <- c(77, 99, -99)

# function that returns true if values in vector are equal to missing_vals. The function takes a vector x, and specified values of missing data
recode_missing <- function(x, missing_vals = c(77, 99, -99)) {
  test <- x %in% missing_vals
  ifelse(test, NA, x)
}

# function that recodes missing values to NA. The function takes a dataframe with variables with missing data, and specified values of missing data
recode_missing_df <- function(df, missing_vals = c(77, 99, -99)) {
  modify(df, ~recode_missing(.x, missing_vals)) # here uses the function created above
}

d2 <- recode_missing_df(d1) # the function strips out variable labels
```

# descriptives check #2 -- Evaluating missingness 
```{r}
describe(d2)
# using describe function I identified that some variables had a lot of missing information:
# q65_p1 = 89/94 responses ("En esta escuela,siento que hay por lo menos un maestro quien está interesado en conocerme")
# q173_3_p1 = 86/94 responses ("¿Qué tan cómodo/ase siente hablando inglés en la ESCUELA de su joven") --> moderator

(sum(is.na(d2))/prod(dim(d2)))*100 # 0.8450984 (this is less than 1% of the data, should I do something about it?)
# Nico's explanation:
#test <- describe(d2)
#(sum(94-test$n))/(94*107)
```

# Data prep: reverse scoring negatively worded items
```{r}
d3 <- d2 %>%
  mutate(q82_p1 = likert_reverse(q82_p1, top = 4, bottom = 1),
         q83_p1 = likert_reverse(q83_p1, top = 4, bottom = 1),
         q131_p1 = likert_reverse(q131_p1, top = 4, bottom = 1),
         q133_p1 = likert_reverse(q133_p1, top = 4, bottom = 1),
         q136_p1 = likert_reverse(q136_p1, top = 4, bottom = 1),
         q84_y1 = likert_reverse(q84_y1, top = 5, bottom = 1),
         q86_y1 = likert_reverse(q86_y1, top = 5, bottom = 1),
         q87_y1 = likert_reverse(q87_y1, top = 5, bottom = 1))
```

```{r}
# d3 %>%
#   haven::write_sav(here("data", "d3.sav"))
```


# dataset with only EFA variables 
```{r}
efa_vars <- d3 %>%
  select(-school_id, -condition, -family_id, -participant_id, -q36_hv1, -q173_3_p1)

describe(efa_vars)
```

# checking assumptions for pearson correlation model
"Departures from normality and linearity are important only because they affect the Pearson product-moment correlation coefficients (r) among measured variables used for computation of EFA results, which, in turn, “can result in misleading EFA findings” (Reise, Waller, & Comrey, 2000, p. 289). Therefore, it is important to investigate and report the distributional properties of the data that might affect the Pearson correlations (Goodwin & Leech, 2006)." (Watkins, 2018, p. 223). 

- Variability
- linearity
- Normality (skew & kurtosis)
- Outliers
- Measurement error: variables with reliabilities > .70 (which computation?)
- Correlation matrix: sizable number of correlations should exceed ±.30 

## using pearson correlation 
# skew & kurtosis: inside the range of -2 to 2 (-2, -1, 0, 1, 2)
# then, look at those variables that appear to be problematic
# then look the bar graph for those problematic variables
# run EFA with them, run without them -- sensitivity analysis -- (if still indicates the same # of factors, decide if leaving or keeping)

## Inspecting normality

# checking for out of range kurtosis & skew variables
```{r}
efa_vars_desc <- data.frame(describe(efa_vars))

efa_vars_desc %>%
  filter(kurtosis < -2 | kurtosis > 2)

efa_vars_desc %>%
  filter(skew < -2 | skew > 2) # these are included above
```

q74_p1 -- "En los últimos tres meses, ¿con qué frecuencia usted ha tenido una conversación con su joven sobre cómo va en sus clases."
1 = Nunca, 2 = Raramente, 3 = A veces, 4 = A menudo.
skew: -2.99
kurtosis: 10.57


# function to visualize all variables in the dataset

code taken from: https://stackoverflow.com/questions/52822840/ggplot2-create-a-barplot-for-every-column-of-a-dataframe 

```{r}
plots <- split.default(efa_vars, names(efa_vars)) %>% 
  map(., setNames, nm = "var") %>% 
  map(., rownames_to_column) %>%
  imap(., ~ {
    ggplot(.x, aes(var)) + 
      geom_bar() +
      labs(title = .y)
    })
```


# inspecting linearity (well, trying)
```{r}
 ggplot(efa_vars, aes(q23_p1, q24_p1)) +
  geom_count() +
  geom_jitter(h = 2, w = 2)

# plots2 <- split.default(scale_1, names(scale_1)) %>% 
#   map(., setNames, nm = "var") %>% 
#   map(., rownames_to_column) %>%
#   imap(., ~ {
#     ggplot(.x, aes(rowname, var)) + 
#       geom_count() + 
#       coord_flip() +
#       labs(title = .y)
#     }) # this is one is not clear
```

# Multivariate and univariate normality tests or all variables
```{r}
mvn(efa_vars, subset = NULL, mvnTest = "hz", univariateTest = "AD") # none of the variables meet the nornmality assumption based on Anderson-Darling test
```

# Multivariate and univariate normality tests for first scale: structure at home
```{r}
scale_1 <- efa_vars %>%
  select(q23_p1:q30_p1)

mvn(scale_1, subset = NULL, mvnTest = "hz", univariateTest = "AD", univariatePlot = "histogram") # univariate plots can be outputted when analyzing scales
```


# issue of sparseness
```{r}
#contingency tables? 
```


--------------------------
## EFA decisions 
--------------------------
```{r}
# Structure at home (8)
scale_1 <- efa_vars %>%
  select(q23_p1:q30_p1)

# Poly corr matrix
poly_scale_1 <- polychoric(scale_1)
poly_scale_1_mat <- data.frame(poly_scale_1$rho)

# scree plot
scree(poly_scale_1_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_1_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
# School-based involvement (10)
scale_2 <- efa_vars %>%
  select(q31_p1:q40_p1)

# Poly corr matrix
poly_scale_2 <- polychoric(scale_2)
poly_scale_2_mat <- data.frame(poly_scale_2$rho)

# scree plot
scree(poly_scale_2_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_2_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
# Parent Belongingness in School (7)
scale_3 <- efa_vars %>%
  select(q47_p1, q48_p1, q51_p1, q52_p1, q53_p1, q59_p1, q60_p1)

# Poly corr matrix
poly_scale_3 <- polychoric(scale_3)
poly_scale_3_mat <- data.frame(poly_scale_3$rho)

# scree plot
scree(poly_scale_3_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_3_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
# Parent Endorsement of School (4)
scale_4 <- efa_vars %>%
  select(q54_p1:q57_p1)

# Poly corr matrix
poly_scale_4 <- polychoric(scale_4)
poly_scale_4_mat <- data.frame(poly_scale_4$rho)

# scree plot
scree(poly_scale_4_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_4_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
# Parent-Teacher Relationship (4) 
scale_5 <- efa_vars %>%
  select(q64_p1:q67_p1)

# Poly corr matrix
poly_scale_5 <- polychoric(scale_5)
poly_scale_5_mat <- data.frame(poly_scale_5$rho)

# scree plot
scree(poly_scale_5_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_5_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Parent-Child Conversations About School (14)
scale_6 <- efa_vars %>%
  select(q68_p1:q81_p1)

# Poly corr matrix
poly_scale_6 <- polychoric(scale_6)
poly_scale_6_mat <- data.frame(poly_scale_6$rho)

# scree plot
scree(poly_scale_6_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_6_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
# Homework Involvement
scale_7 <- efa_vars %>%
  select(q82_p1:q98_p1)

# Poly corr matrix
poly_scale_7 <- polychoric(scale_7)
poly_scale_7_mat <- data.frame(poly_scale_7$rho)

# scree plot
scree(poly_scale_7_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_7_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Parent’s Value and Support of Education (6)
scale_8 <- efa_vars %>%
  select(q105_p1:q110_p1)

# Poly corr matrix
poly_scale_8 <- polychoric(scale_8)
poly_scale_8_mat <- data.frame(poly_scale_8$rho)

# scree plot
scree(poly_scale_8_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_8_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Appropriate Discipline (7)
scale_9 <- efa_vars %>%
  select(q128_p1:q131_p1, q133_p1, q134_p1, q136_p1)

# Poly corr matrix
poly_scale_9 <- polychoric(scale_9)
poly_scale_9_mat <- data.frame(poly_scale_9$rho)

# scree plot
scree(poly_scale_9_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_9_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Monitoring (5)
scale_10 <- efa_vars %>%
  select(q137_p1, q139_p1, q140_p1, q143_p1, q144_p1)

# Poly corr matrix
poly_scale_10 <- polychoric(scale_10)
poly_scale_10_mat <- data.frame(poly_scale_10$rho)

# scree plot
scree(poly_scale_10_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_10_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Family-School Communication (6)
scale_11 <- efa_vars %>%
  select(q18_p1, q19_p1, q41_p1, q43_p1, q50_p1, q62_p1)

# Poly corr matrix
poly_scale_11 <- polychoric(scale_11)
poly_scale_11_mat <- data.frame(poly_scale_11$rho)

# scree plot
scree(poly_scale_11_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_11_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Problem Solving with Educators (4)
scale_12 <- efa_vars %>%
  select(q44_p1, q58_p1, q63_p1, q126_p1)

# Poly corr matrix
poly_scale_12 <- polychoric(scale_12)
poly_scale_12_mat <- data.frame(poly_scale_12$rho)

# scree plot
scree(poly_scale_12_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_12_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```

```{r}
#Students’ School Engagement (9)
scale_13 <- efa_vars %>%
  select(q83_y1:q91_y1)

# Poly corr matrix
poly_scale_13 <- polychoric(scale_13)
poly_scale_13_mat <- data.frame(poly_scale_13$rho)

# scree plot
scree(poly_scale_13_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

# EFA
fa(poly_scale_13_mat, n.obs = NA, rotate = "none", fm = "uls", cor = "poly")
```
Determining the number of factors in EFA:

"A number of analytical processes have been used to determine
the number of factors to retain from a list of items, and it is
beyond the scope of this paper to describe all of them. For
scale development, commonly available methods to determine
the number of factors to retain include a scree plot (85), the
variance explained by the factor model, and the pattern of factor
loadings (2). Where feasible, researchers could also assess the
optimal number of factors to be drawn fromthe list of items using
either parallel analysis (86), minimum average partial procedure
(87), or the Hull method (88, 89)." (Boateng et al., 2018)

DD also suggested parallewl analysis... Maybe use it when in doubt? For HW involvement? 

---------------------------------------------
Psych package info:
" For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin KMO test may be found from the correlation matrix or data matrix using the KMO function. Similarly, the Bartlett’s test of Sphericity may be found using the cortest.bartlett function"


Model fit indices:

"RMSEA is an absolute fit index, in that it assesses how far a hypothesized model is from a perfect model. On the contrary, CFI and TLI are incremental fit indices that compare the fit of a hypothesized model with that of a baseline model (i.e., a model with the worst fit)" (Xia & Yang, p. 309)

Hu and Bentler (1999) suggested relatively good model–data fit:
RMSEA smaller than .06 
CFI and TLI larger than .95 

But these estimates are for continuous data, using normal-theory maximum likelihood (ML). 

Conclusion from (Xia & Yang, p. 421)
"Given that the DWLS and ULS fit indices tend to show a better model–data fit evaluation than do ML fit indices when the same misspecified model is analyzed, we argue that surpassing a set of cutoff values should not serve as the only justification for the acceptance of a model. It would be more appropriate to consider RMSEA, CFI, and TLI as diagnostic tools for model improvement." 

---- Qs ----
Q: Are model fit indices used in EFA? Still not super clear. DD said will send some info.

reliability --- alpha -- can be done with ordinal scales?

DD: polychoric may be robust to non-normality


test to look at input hsquared