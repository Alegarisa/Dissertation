---
title: "analyses"
author: "Alejandra Garcia Isaza"
date: "4/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rio)
library(here)
library(tidyverse)
library(haven)
library(janitor)
library(knitr)
library(surveytoolbox)
library(sjPlot)
library(kableExtra)
library(psych)
library(sjmisc)
library(MVN)

theme_set(theme_minimal())
```

# Loading the dataset
```{r}
d <- read_sav(here("data", "parent_hv1_youth_w1.sav"))
```

# only analysis variables

## id variables
school_id, condition, family_id, participant_id

## scales variables
- Structure at home (8)
select(q23_p1:q30_p1)

- School-Based Involvement (10)
select(q31_p1:q40_p1)

- Parent Belongingness in School (7)
select(q47_p1, q48_p1, q51_p1, q52_p1, q53_p1, q59_p1, q60_p1)

- Parent Endorsement of School (4)
select(q54_p1:q57_p1)

- Parent-Teacher Relationship (4) 
select(q64_p1:q67_p1)

- Parent-Child Conversations About School (14)
select(q68_p1:q81_p1)

- Homework Involvement (17)
select(q82_p1:q98_p1)

- Parent’s Value and Support of Education (6)
select(q105_p1:q110_p1)

- Appropriate Discipline (7)
select(q128_p1:q131_p1, q133_p1, q134_p1, q136_p1)

- Monitoring (5)
select(q137_p1, q139_p1, q140_p1, q143_p1, q144_p1)

- Family-School Communication (6)
select(q18_p1, q19_p1, q41_p1, q43_p1, q50_p1, q62_p1)

- Problem Solving with Educators (4)
select(q44_p1, q58_p1, q63_p1, q126_p1)

## outcome variable
- Students’ School Engagement (9)
select(q83_y1:q91_y1)

## moderators
- Ed level (1 - 11)
q36_hv1

- ENG comfort in youth school (1 - 5, 77, 99)
q173_3_p1

```{r}
d1 <- d %>%
  select(school_id, condition, family_id, participant_id, q23_p1:q30_p1, q31_p1:q40_p1, q47_p1, q48_p1, q51_p1, q52_p1, q53_p1, q59_p1, q60_p1, q54_p1:q57_p1, q64_p1:q67_p1, q68_p1:q81_p1, q82_p1:q98_p1, q105_p1:q110_p1, q128_p1:q131_p1, q133_p1, q134_p1, q136_p1, q137_p1, q139_p1, q140_p1, q143_p1, q144_p1, q18_p1, q19_p1, q41_p1, q43_p1, q50_p1, q62_p1, q44_p1, q58_p1, q63_p1, q126_p1, q36_hv1, q173_3_p1, q83_y1:q91_y1) # double check this, evaluate missing data 
```

# descriptives check 
```{r}
describe(d1)# using describe function I identified that missing values in the dataset were -99, 99, 77
```


# recoding missing variables as N/A
```{r include=FALSE}
# recoding missing values as N/A with function

# vector with missing values in dataset
missing_vals <- c(77, 99, -99)

# function that returns true if values in vector are equal to missing_vals. The function takes a vector x, and specified values of missing data
recode_missing <- function(x, missing_vals = c(77, 99, -99)) {
  test <- x %in% missing_vals
  ifelse(test, NA, x)
}

# function that recodes missing values to NA. The function takes a dataframe with variables with missing data, and specified values of missing data
recode_missing_df <- function(df, missing_vals = c(77, 99, -99)) {
  modify(df, ~recode_missing(.x, missing_vals)) # here uses the function created above
}

d2 <- recode_missing_df(d1) # the function strips out variable labels
```

# descriptives check #2 -- Evaluating missingness 
```{r}
describe(d2)
# using describe function I identified that some variables had a lot of missing information:
# q65_p1 = 89/94 responses ("En esta escuela,siento que hay por lo menos un maestro quien está interesado en conocerme")
# q173_3_p1 = 86/94 responses ("¿Qué tan cómodo/ase siente hablando inglés en la ESCUELA de su joven") --> moderator

(sum(is.na(d2))/prod(dim(d2)))*100 # 0.8450984 (this is less than 1% of the data, should I do something about it?)
# Nico's explanation:
#test <- describe(d2)
#(sum(94-test$n))/(94*107)
```

# Data prep: reverse scoring negatively worded items
```{r}
d3 <- d2 %>%
  mutate(q82_p1 = likert_reverse(q82_p1, top = 4, bottom = 1),
         q83_p1 = likert_reverse(q83_p1, top = 4, bottom = 1),
         q131_p1 = likert_reverse(q131_p1, top = 4, bottom = 1),
         q133_p1 = likert_reverse(q133_p1, top = 4, bottom = 1),
         q136_p1 = likert_reverse(q136_p1, top = 4, bottom = 1),
         q84_y1 = likert_reverse(q84_y1, top = 5, bottom = 1),
         q86_y1 = likert_reverse(q86_y1, top = 5, bottom = 1),
         q87_y1 = likert_reverse(q87_y1, top = 5, bottom = 1))
```

```{r}
d3 %>%
  haven::write_sav(here("data", "d3.sav"))
```


# dataset with only EFA variables 
```{r}
efa_vars <- d3 %>%
  select(-school_id, -condition, -family_id, -participant_id, -q36_hv1, -q173_3_p1)

describe(efa_vars)
```

# checking assumptions for pearson correlation model
"Departures from normality and linearity are important only because they affect the Pearson product-moment correlation coefficients (r) among measured variables used for computation of EFA results, which, in turn, “can result in misleading EFA findings” (Reise, Waller, & Comrey, 2000, p. 289). Therefore, it is important to investigate and report the distributional properties of the data that might affect the Pearson correlations (Goodwin & Leech, 2006)." (Watkins, 2018, p. 223). 

- Variability
- linearity
- Normality (skew & kurtosis)
- Outliers
- Measurement error: variables with reliabilities > .70 (which computation?)
- Correlation matrix: sizable number of correlations should exceed ±.30 

## using pearson correlation 
# skew & kurtosis: inside the range of -2 to 2 (-2, -1, 0, 1, 2)
# then, look at those variables that appear to be problematic
# then look the bar graph for those problematic variables
# run EFA with them, run without them -- sensitivity analysis -- (if still indicates the same # of factors, decide if leaving or keeping)

```{r}
# q74_p1 -- "En los últimos tres meses, ¿con qué frecuencia usted ha tenido una conversación con su joven sobre cómo va en sus clases."
# 1 = Nunca, 2 = Raramente, 3 = A veces, 4 = A menudo. 
# skew: -2.99	
# kurtosis: 10.57

efa_vars$q74_p1 # checking that no weird number is there

ggplot(efa_vars, aes(q74_p1)) +
geom_bar()
```

# checking for out of range kurtosis & skew variables
```{r}
efa_vars_desc <- data.frame(describe(efa_vars))

efa_vars_desc %>%
  filter(kurtosis < -2 | kurtosis > 2)

efa_vars_desc %>%
  filter(skew < -2 | skew > 2) # these are included above
```

# inspecting normality
```{r}
ggplot(efa_vars, aes(q23_p1)) +
geom_bar() # to visualize univariate discrete

# check for normality looking at bar plots (101 vars that will go in EFA) -- find a function to do this
```

#trying a function to visualize all variables

code taken from: https://stackoverflow.com/questions/52822840/ggplot2-create-a-barplot-for-every-column-of-a-dataframe 
```{r}

plots <- split.default(efa_vars, names(efa_vars)) %>% 
  map(., setNames, nm = "var") %>% 
  map(., rownames_to_column) %>%
  imap(., ~ {
    ggplot(.x, aes(var)) + 
      geom_bar() +
      labs(title = .y)
    })

plots
```


# inspecting linearity (well, trying)
```{r}
 ggplot(efa_vars, aes(q23_p1, q24_p1)) +
  geom_count() +
  geom_jitter(h = 2, w = 2)
```

```{r}
mvn(scale_1, subset = NULL, mvnTest = "hz", univariateTest = "AD", univariatePlot = "histogram") # none of the variables meet the nornmality assumption based on Anderson-Darling test
```



# issue of sparseness
```{r}
#contingency tables? 
```


--------------------------
## EFA decisions 
--------------------------
```{r}
# Structure at home
scale_1 <- efa_vars %>%
  select(q23_p1:q30_p1)

round(cor(scale_1, method = "pearson", use = "complete.obs"), digits = 2) # pearson corr
```

```{r}
poly_scale_1 <- polychoric(scale_1) # applying poly function to scale 1

poly_scale_1 # RHO is the polychoric matrix, TAU is the "items difficulties" (?)

poly_scale_1_mat <- data.frame(poly_scale_1$rho) # need to create a datafranme with correlation matrix to use it in scree function 
```

```{r}
scree(poly_scale_1_mat,factors=TRUE,pc=FALSE,main="Scree plot",hline=NULL,add=FALSE)

```

# how to ask for estimation method ULS or ML --- answer: fm = "uls"
```{r}
# this is the code copied from psych package documentation 
fa(poly_scale_1_mat, nfactors=1, n.obs = NA, n.iter=1, rotate = "oblimin", scores = "regression", residuals=FALSE, SMC=TRUE, covar=FALSE, missing=FALSE, impute="median", min.err = 0.001, max.iter = 50, symmetric=TRUE, warnings=TRUE, fm="uls", alpha=.1, p=.05, oblique.scores=FALSE, np.obs=NULL,use="pairwise", cor="poly", correct=.5, weight=NULL, n.rotations=1, hyper=.15)
```



# need to check if adding cor = "poly" is duplicative if the polychoric correlation matrix is provided instead of the dataframe with the items. Test both and see if results differ. 
```{r}
fa(poly_scale_1_mat, n.obs = NA, rotate = "none", fm = "uls")

```

```{r}
fa(poly_scale_1_mat, nfactors = 1, n.obs = NA, n.iter = 1, rotate = "none", fm = "uls", cor = "poly", SMC=TRUE)

```

Psych package info:
" For those who like SPSS type output, the measure of factoring adequacy known as the Kaiser-Meyer-Olkin KMO test may be found from the correlation matrix or data matrix using the KMO function. Similarly, the Bartlett’s test of Sphericity may be found using the cortest.bartlett function"


Model fit indices:

"RMSEA is an absolute fit index, in that it assesses how far a hypothesized model is from a perfect model. On the contrary, CFI and TLI are incremental fit indices that compare the fit of a hypothesized model with that of a baseline model (i.e., a model with the worst fit)" (Xia & Yang, p. 309)

Hu and Bentler (1999) suggested relatively good model–data fit:
RMSEA smaller than .06 
CFI and TLI larger than .95 

But these estimates are for continuous data, using normal-theory maximum likelihood (ML). 

Conclusion from (Xia & Yang, p. 421)
"Given that the DWLS and ULS fit indices tend to show a better model–data fit evaluation than do ML fit indices when the same misspecified model is analyzed, we argue that surpassing a set of cutoff values should not serve as the only justification for the acceptance of a model. It would be more appropriate to consider RMSEA, CFI, and TLI as diagnostic tools for model improvement." 

Q: Are model fit indices used in EFA? 

MVN package
anderson-darlin test for normality

polychoric may be robust to non-normality

reliability --- alpha 
test to look at input hsquared